{
  "metadata": {
    "name": "test",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.SparkSession\n\nval spark\u003d SparkSession\n    .builder()\n    .config(\"spark.jars\", \"/opt/jar/spark-xml_2.12-0.9.0.jar\")\n    .appName(\"spark get xml data from hdfs\")\n    .getOrCreate()"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import com.databricks.spark.xml._\n\nval rawXML \u003d spark.read.option(\"rowTag\", \"MedlineCitation\").xml(\"hdfs://namenode:8020/medline/medsamp2016a.xml\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "rawXML.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import spark.implicits._\nval meshHeadlingList \u003d rawXML.select(\"MeshHeadingList.MeshHeading\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "meshHeadlingList.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "meshHeadlingList.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "meshHeadlingList.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val MeshHeadlingElems \u003d meshHeadlingList.withColumn(\"data\", explode($\"MeshHeading\")).select(\"data\")"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "MeshHeadlingElems.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val descriptorName \u003d MeshHeadlingElems.select(MeshHeadlingElems.col(\"data.DescriptorName\"))\ndescriptorName.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val parsedDF \u003d descriptorName.select(descriptorName.col(\"DescriptorName._MajorTopicYN\"),\n                                    descriptorName.col(\"DescriptorName._Type\"),\n                                    descriptorName.col(\"DescriptorName._UI\"),\n                                    descriptorName.col(\"DescriptorName._VALUE\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "parsedDF.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val majorTopic \u003d parsedDF.filter(col(\"_MajorTopicYN\") \u003d\u003d\u003d \"Y\")"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val toppicDist \u003d majorTopic.groupBy(\"_VALUE\").count()"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "toppicDist.orderBy(desc(\"count\")).show()"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "majorTopic.show(false)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val majorTopicVal \u003d majorTopic.select(col(\"_VALUE\").as(\"topic\")).cache()"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "majorTopicVal.show(false)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val topics \u003d majorTopicVal.select(\"topic\").rdd.map(el \u003d\u003e el.getString(0).split(\",\").toList)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val onlyTopics \u003d  topics.flatMap(mesh \u003d\u003e mesh).toDF(\"topic\")"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val topicPairs \u003d topics.flatMap(t \u003d\u003e {t.sorted.combinations(2)}).toDF(\"pairs\")\ntopicPairs.createOrReplaceTempView(\"topic_pairs\")\nval coccurs \u003d spark.sql(\"\"\"\n    SELECT pairs, COUNT(*) cnt\n    FROM topic_pairs\n    GROUP BY pairs\"\"\")\n"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "cooccurs.createOrReplaceTempView(\"cooccurs\")\nspark.sql(\"\"\"\n    SELECT pairs, cnt\n    FROM cooccurs\n    ORDER BY cnt DESC\n    LIMIT 10\"\"\").collect().foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import java.nio.charset.StandardCharsets\nimport java.security.MessageDigest\n\ndef hashID(str: String): Long \u003d {\n    val bytes \u003d MessageDigest.getInstance(\"MD5\").digest(str.getBytes(StandardCharsets.UTF_8))\n    (bytes(0) \u0026 0xFFL) |\n    ((bytes(1) \u0026 0xFFL) \u003c\u003c 8)  |\n    ((bytes(2) \u0026 0xFFL) \u003c\u003c 16) |\n    ((bytes(3) \u0026 0xFFL) \u003c\u003c 24) | \n    ((bytes(4) \u0026 0xFFL) \u003c\u003c 32) |\n    ((bytes(5) \u0026 0xFFL) \u003c\u003c 40) |\n    ((bytes(6) \u0026 0xFFL) \u003c\u003c 48) |\n    ((bytes(7) \u0026 0xFFL) \u003c\u003c 56)\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.Row\n\nval vertices \u003d onlyTopics.map{ case Row(topic: String) \u003d\u003e (hashID(topic), topic) }.toDF(\"hash\", \"topic\")\nvertices.show(false)"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.graphx._\n\nval edges \u003d cooccurs.map{ case Row(pairs: Seq[_], cnt: Long) \u003d\u003e\n    val ids \u003d pairs.map(_.toString).map(hashID).sorted\n    Edge(ids(0), ids(1), cnt)\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val vertexRDD \u003d vertices.rdd.map{\n    case Row(hash: Long, topic: String) \u003d\u003e (hash, topic)\n}\nval topicGraph \u003d Graph(vertexRDD, edges.rdd)\ntopicGraph.cache()"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val connectedComponentGraph \u003d topicGraph.connectedComponents()"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val componentDF \u003d connectedComponentGraph.vertices.toDF(\"vid\", \"cid\")\ncomponentDF.show(false)"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val componentCounts \u003d componentDF.groupBy(\"cid\").count()\ncomponentCounts.count()"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "z.show(componentCounts.orderBy(desc(\"count\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions.split\n\nval topicSplittopicSplit \u003d majorTopicVal.withColumn(\"topic_split\", split(col(\"topic\"), \",\"))\ntopicSplit.withColumn(\"size\", size(col(\"topic_split\"))).groupBy(\"topic_split\", \"size\").count().filter(col(\"size\") \u003d\u003d\u003d 2).orderBy(desc(\"count\")).show(false)"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val topicSplitExp \u003d topicSplit.withColumn(\"topic1\", explode(col(\"topic_split\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val topicTrimmed \u003d topicSplitExp.withColumn(\"topic1\", trim(col(\"topic1\")))"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "topicTrimmed.show"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val topics \u003d topicTrimmed.select(\"topic\", \"topic1\")\ntopics.show(false)"
    }
  ]
}